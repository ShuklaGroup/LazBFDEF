{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] datasets evaluate"
      ],
      "metadata": {
        "id": "OmMRCa8hCz30"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CEsypwFwCjld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QNQqU4d2AXW6"
      },
      "outputs": [],
      "source": [
        "# Performs supervised fine-tuning LazBF-ESM and LazDEF-ESM\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.utils import resample\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.metrics import ndcg_score\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, normalize\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.utils import resample\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "import matplotlib.ticker as ticker\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import ndcg_score\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.svm import SVR, SVC\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "from evaluate import load\n",
        "from datasets import load_metric\n",
        "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load sequences from csv\n",
        "df = pd.read_csv('./drive/MyDrive/Data/LazBF_sequences.csv')\n",
        "LazBF_sequences = df['sequences'].tolist()\n",
        "LazBF_labels = df['labels'].tolist()\n",
        "\n",
        "df = pd.read_csv('./drive/MyDrive/Data/LazBF_sample.csv')\n",
        "LazBF_sample = df['sequences'].tolist()\n",
        "LazBF_sample_labels = df['labels'].tolist()\n",
        "\n",
        "df = pd.read_csv('./drive/MyDrive/Data/LazDEF_sequences.csv')\n",
        "LazDEF_sequences = df['sequences'].tolist()\n",
        "LazDEF_labels = df['labels'].tolist()\n",
        "\n",
        "df = pd.read_csv('./drive/MyDrive/Data/LazDEF_sample.csv')\n",
        "LazDEF_sample = df['sequences'].tolist()\n",
        "LazDEF_sample_labels = df['labels'].tolist()\n",
        "\n",
        "df = pd.read_csv('./drive/MyDrive/Data/LazBCDEF_sequences.csv')\n",
        "LazBCDEF_sequences = df['sequences'].tolist()\n",
        "LazBCDEF_labels = df['labels'].tolist()\n",
        "\n",
        "df = pd.read_csv('./drive/MyDrive/Data/LazBCDEF_sample.csv')\n",
        "LazBCDEF_sample = df['sequences'].tolist()\n",
        "LazBCDEF_sample_labels = df['labels'].tolist()\n",
        "\n",
        "LazBF_ft = AutoModelForSequenceClassification.from_pretrained('facebook/esm2_t33_650M_UR50D').to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
        "\n",
        "# Supervised fine-tune LazBF-ESM\n",
        "train = Dataset.from_dict(tokenizer(LazBF_sequences))\n",
        "train = train.add_column(\"labels\", LazBF_labels)\n",
        "train = train.shuffle(seed=16)\n",
        "\n",
        "test = Dataset.from_dict(tokenizer(LazBF_sample))\n",
        "test = test.add_column(\"labels\", LazBF_sample_labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./drive/MyDrive/Models/LazBF_ft\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=256,\n",
        "    per_device_eval_batch_size=256,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "    gradient_accumulation_steps=2,\n",
        ")\n",
        "\n",
        "metric = load_metric('accuracy')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=LazBF_ft,\n",
        "    args=training_args,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=test,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LazDEF_ft = AutoModelForSequenceClassification.from_pretrained('facebook/esm2_t33_650M_UR50D').to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
        "\n",
        "# Supervised fine-tune LazDEF-ESM\n",
        "train = Dataset.from_dict(tokenizer(LazDEF_sequences))\n",
        "train = train.add_column(\"labels\", LazDEF_labels)\n",
        "train = train.shuffle(seed=42)\n",
        "\n",
        "test = Dataset.from_dict(tokenizer(LazDEF_sample))\n",
        "test = test.add_column(\"labels\", LazDEF_sample_labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./drive/MyDrive/Models/LazDEF_ft\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=256,\n",
        "    per_device_eval_batch_size=256,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "    gradient_accumulation_steps=2,\n",
        ")\n",
        "\n",
        "metric = load_metric('accuracy')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=LazDEF_ft,\n",
        "    args=training_args,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=test,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "MN5spmRECHxG",
        "outputId": "28bea8eb-7f71-4563-bf22-deef5a27415a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-2-c15a07670e79>:27: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric('accuracy')\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1675' max='2441' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1675/2441 49:36 < 22:43, 0.56 it/s, Epoch 0.69/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2441' max='2441' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2441/2441 1:13:41, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.031600</td>\n",
              "      <td>0.026966</td>\n",
              "      <td>0.991960</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2441, training_loss=0.04525823839274355, metrics={'train_runtime': 4424.5114, 'train_samples_per_second': 282.517, 'train_steps_per_second': 0.552, 'total_flos': 7.322542384909824e+16, 'train_loss': 0.04525823839274355, 'epoch': 0.999795207864018})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LazBCDEF_ft = AutoModelForSequenceClassification.from_pretrained('facebook/esm2_t33_650M_UR50D').to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
        "\n",
        "# Supervised fine-tune LazDEF-ESM\n",
        "train = Dataset.from_dict(tokenizer(LazBCDEF_sequences, padding='longest'))\n",
        "train = train.add_column(\"labels\", LazBCDEF_labels)\n",
        "train = train.shuffle(seed=42)\n",
        "\n",
        "test = Dataset.from_dict(tokenizer(LazBCDEF_sample, padding='longest'))\n",
        "test = test.add_column(\"labels\", LazBCDEF_sample_labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./drive/MyDrive/Models/LazBCDEF_ft\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=256,\n",
        "    per_device_eval_batch_size=256,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "    gradient_accumulation_steps=2,\n",
        ")\n",
        "\n",
        "metric = load_metric('accuracy')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=LazBCDEF_ft,\n",
        "    args=training_args,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=test,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "tQgt0Yk0CPFG",
        "outputId": "b00b19ac-ffc8-4a03-eef3-4d6142cbb8f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2441' max='2441' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2441/2441 1:04:51, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.121600</td>\n",
              "      <td>0.120519</td>\n",
              "      <td>0.957620</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2441, training_loss=0.13213378081112698, metrics={'train_runtime': 3894.0005, 'train_samples_per_second': 321.007, 'train_steps_per_second': 0.627, 'total_flos': 6.346203400255181e+16, 'train_loss': 0.13213378081112698, 'epoch': 0.999795207864018})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}