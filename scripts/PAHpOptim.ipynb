{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S_aQOmanU2NW"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import torch\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.utils import resample\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.lines as mlines\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.metrics import ndcg_score\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, normalize\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.utils import resample\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.lines as mlines\n",
        "# import matplotlib.ticker as ticker\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import ndcg_score\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.svm import SVR, SVC\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "# from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\n",
        "# from datasets import Dataset\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import precision_score, recall_score, roc_auc_score, accuracy_score\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#title Load sequences from csv\n",
        "df = pd.read_csv('./drive/MyDrive/LazBF_sequences.csv')\n",
        "LazBF_sequences = df['sequences'].tolist()\n",
        "LazBF_labels = df['labels'].tolist()\n",
        "\n",
        "df = pd.read_csv('./drive/MyDrive/LazBF_sample.csv')\n",
        "LazBF_sample = df['sequences'].tolist()\n",
        "LazBF_sample_labels = df['labels'].tolist()\n",
        "\n",
        "df = pd.read_csv('./drive/MyDrive/LazDEF_sequences.csv')\n",
        "LazDEF_sequences = df['sequences'].tolist()\n",
        "LazDEF_labels = df['labels'].tolist()\n",
        "\n",
        "df = pd.read_csv('./drive/MyDrive/LazDEF_sample.csv')\n",
        "LazDEF_sample = df['sequences'].tolist()\n",
        "LazDEF_sample_labels = df['labels'].tolist()\n",
        "\n",
        "#title Load Embs\n",
        "lazbf_mlm_none = np.load(\"./drive/MyDrive/Embeddings/LazBF_mlm_none.npy\")\n",
        "lazdef_mlm_none = np.load(\"./drive/MyDrive/Embeddings/LazDEF_mlm_none.npy\")\n",
        "\n",
        "lazbf_mlm_lazbf = np.load(\"./drive/MyDrive/Embeddings/LazBF_mlm_LazBF.npy\")\n",
        "lazdef_mlm_lazbf = np.load(\"./drive/MyDrive/Embeddings/LazDEF_mlm_LazBF.npy\")\n",
        "\n",
        "lazbf_mlm_lazdef = np.load(\"./drive/MyDrive/Embeddings/LazBF_mlm_LazDEF.npy\")\n",
        "lazdef_mlm_lazdef = np.load(\"./drive/MyDrive/Embeddings/LazDEF_mlm_LazDEF.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXvolqrf8xwc"
      },
      "outputs": [],
      "source": [
        "lazbf_mlm_PA = np.load(\"./drive/MyDrive/LazBF_mlm_PA.npy\")\n",
        "lazdef_mlm_PA = np.load(\"./drive/MyDrive/LazDEF_mlm_PA.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ODzbQL2Zj45",
        "outputId": "9e5fc84a-42a0-498e-dcba-f8636750793a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimization highn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "40 fits failed out of a total of 80.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [    nan 0.9021      nan 0.90162     nan 0.9021      nan 0.9018      nan\n",
            " 0.90284     nan 0.90176     nan 0.90154     nan 0.90244]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best paramters for Logistic regression {'classifier__C': 5, 'classifier__penalty': 'l2'}\n",
            "Best model for Logistic regression Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', LogisticRegression(C=5))])\n",
            "Best paramters for KNN classif {'classifier__n_neighbors': 50, 'classifier__weights': 'uniform'}\n",
            "Best model for KNN classif Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', KNeighborsClassifier(n_neighbors=50))])\n",
            "Best paramters for RF {'classifier__criterion': 'entropy', 'classifier__n_estimators': 500}\n",
            "Best model for RF Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier',\n",
            "                 RandomForestClassifier(criterion='entropy',\n",
            "                                        n_estimators=500))])\n",
            "Best paramters for AdaBoost {'classifier__learning_rate': 1, 'classifier__n_estimators': 500}\n",
            "Best model for AdaBoost Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier',\n",
            "                 AdaBoostClassifier(learning_rate=1, n_estimators=500))])\n"
          ]
        }
      ],
      "source": [
        "optX = lazbf_mlm_PA\n",
        "opty = LazBF_sample_labels\n",
        "print(\"Optimization highn\")\n",
        "\n",
        "model_list = [LogisticRegression, KNeighborsClassifier, RandomForestClassifier, AdaBoostClassifier, SVC, MLPClassifier]\n",
        "model_names = ['Logistic regression', 'KNN classif', 'RF', 'AdaBoost', 'SVC', 'MLP']\n",
        "parameters_list = [\n",
        "    {'classifier__penalty': (\"l1\", \"l2\", \"elasticnet\", None), 'classifier__C':[0.1, 1, 5, 10]},\n",
        "    {'classifier__n_neighbors': [5, 10, 25, 50], 'classifier__weights': ('uniform', 'distance')},\n",
        "    {'classifier__n_estimators': [50, 100, 200, 500], 'classifier__criterion': ('gini', 'entropy', 'log_loss')},\n",
        "    {'classifier__n_estimators': [50, 100, 200, 500], 'classifier__learning_rate': [0.1, 1, 5, 10]},\n",
        "    {'classifier__kernel':('linear', 'rbf'), 'classifier__C':[0.1, 1, 5, 10]},\n",
        "    {'classifier__hidden_layer_sizes': [50, 100, 500, 750, 1000], 'classifier__activation':('tanh', 'relu')},\n",
        "]\n",
        "\n",
        "for model, name, parameters in zip(model_list, model_names, parameters_list):\n",
        "    steps = [\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('pca', PCA(n_components=50)),\n",
        "        ('classifier', model())\n",
        "    ]\n",
        "    pipeline = Pipeline(steps)\n",
        "    grid_search = GridSearchCV(pipeline, parameters, scoring='accuracy')\n",
        "    grid_search.fit(optX, opty)\n",
        "    print(f'Best paramters for {name} {grid_search.best_params_}')\n",
        "    print(f'Best model for {name} {grid_search.best_estimator_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAfY3EngcWsj",
        "outputId": "c425eedc-ddce-4ad9-810d-ac99cf8f4c2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimization highn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "40 fits failed out of a total of 80.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [    nan 0.82592     nan 0.82612     nan 0.82638     nan 0.82624     nan\n",
            " 0.8258      nan 0.8257      nan 0.82572     nan 0.82614]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best paramters for Logistic regression {'classifier__C': 1, 'classifier__penalty': 'l2'}\n",
            "Best model for Logistic regression Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', LogisticRegression(C=1))])\n",
            "Best paramters for KNN classif {'classifier__n_neighbors': 50, 'classifier__weights': 'uniform'}\n",
            "Best model for KNN classif Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', KNeighborsClassifier(n_neighbors=50))])\n",
            "Best paramters for RF {'classifier__criterion': 'log_loss', 'classifier__n_estimators': 500}\n",
            "Best model for RF Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier',\n",
            "                 RandomForestClassifier(criterion='log_loss',\n",
            "                                        n_estimators=500))])\n"
          ]
        }
      ],
      "source": [
        "optX = lazdef_mlm_PA\n",
        "opty = LazDEF_sample_labels\n",
        "print(\"Optimization highn\")\n",
        "\n",
        "model_list = [LogisticRegression, KNeighborsClassifier, RandomForestClassifier, AdaBoostClassifier, SVC, MLPClassifier]\n",
        "model_names = ['Logistic regression', 'KNN classif', 'RF', 'AdaBoost', 'SVC', 'MLP']\n",
        "parameters_list = [\n",
        "    {'classifier__penalty': (\"l1\", \"l2\", \"elasticnet\", None), 'classifier__C':[0.1, 1, 5, 10]},\n",
        "    {'classifier__n_neighbors': [5, 10, 25, 50], 'classifier__weights': ('uniform', 'distance')},\n",
        "    {'classifier__n_estimators': [50, 100, 200, 500], 'classifier__criterion': ('gini', 'entropy', 'log_loss')},\n",
        "    {'classifier__n_estimators': [50, 100, 200, 500], 'classifier__learning_rate': [0.1, 1, 5, 10]},\n",
        "    {'classifier__kernel':('linear', 'rbf'), 'classifier__C':[0.1, 1, 5, 10]},\n",
        "    {'classifier__hidden_layer_sizes': [50, 100, 500, 750, 1000], 'classifier__activation':('tanh', 'relu')},\n",
        "]\n",
        "\n",
        "for model, name, parameters in zip(model_list, model_names, parameters_list):\n",
        "    steps = [\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('pca', PCA(n_components=50)),\n",
        "        ('classifier', model())\n",
        "    ]\n",
        "    pipeline = Pipeline(steps)\n",
        "    grid_search = GridSearchCV(pipeline, parameters, scoring='accuracy')\n",
        "    grid_search.fit(optX, opty)\n",
        "    print(f'Best paramters for {name} {grid_search.best_params_}')\n",
        "    print(f'Best model for {name} {grid_search.best_estimator_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEbghT0KVTNN",
        "outputId": "628ea964-7538-4b5d-b0ec-724d3c44a6cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimization LOWN\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "40 fits failed out of a total of 80.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [ nan 0.84  nan 0.82  nan 0.85  nan 0.82  nan 0.85  nan 0.82  nan 0.83\n",
            "  nan 0.82]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best paramters for Logistic regression {'classifier__C': 1, 'classifier__penalty': 'l2'}\n",
            "Best model for Logistic regression Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', LogisticRegression(C=1))])\n",
            "Best paramters for KNN classif {'classifier__n_neighbors': 10, 'classifier__weights': 'uniform'}\n",
            "Best model for KNN classif Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', KNeighborsClassifier(n_neighbors=10))])\n",
            "Best paramters for RF {'classifier__criterion': 'entropy', 'classifier__n_estimators': 50}\n",
            "Best model for RF Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier',\n",
            "                 RandomForestClassifier(criterion='entropy', n_estimators=50))])\n",
            "Best paramters for AdaBoost {'classifier__learning_rate': 1, 'classifier__n_estimators': 50}\n",
            "Best model for AdaBoost Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', AdaBoostClassifier(learning_rate=1))])\n",
            "Best paramters for SVC {'classifier__C': 0.1, 'classifier__kernel': 'linear'}\n",
            "Best model for SVC Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', SVC(C=0.1, kernel='linear'))])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best paramters for MLP {'classifier__activation': 'relu', 'classifier__hidden_layer_sizes': 750}\n",
            "Best model for MLP Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', MLPClassifier(hidden_layer_sizes=750))])\n"
          ]
        }
      ],
      "source": [
        "optX = lazbf_mlm_PA\n",
        "opty = LazBF_sample_labels\n",
        "print(\"Optimization LOWN\")\n",
        "\n",
        "model_list = [LogisticRegression, KNeighborsClassifier, RandomForestClassifier, AdaBoostClassifier, SVC, MLPClassifier]\n",
        "model_names = ['Logistic regression', 'KNN classif', 'RF', 'AdaBoost', 'SVC', 'MLP']\n",
        "parameters_list = [\n",
        "    {'classifier__penalty': (\"l1\", \"l2\", \"elasticnet\", None), 'classifier__C':[0.1, 1, 5, 10]},\n",
        "    {'classifier__n_neighbors': [5, 10, 25, 50], 'classifier__weights': ('uniform', 'distance')},\n",
        "    {'classifier__n_estimators': [50, 100, 200, 500], 'classifier__criterion': ('gini', 'entropy', 'log_loss')},\n",
        "    {'classifier__n_estimators': [50, 100, 200, 500], 'classifier__learning_rate': [0.1, 1, 5, 10]},\n",
        "    {'classifier__kernel':('linear', 'rbf'), 'classifier__C':[0.1, 1, 5, 10]},\n",
        "    {'classifier__hidden_layer_sizes': [50, 100, 500, 750, 1000], 'classifier__activation':('tanh', 'relu')},\n",
        "]\n",
        "\n",
        "for model, name, parameters in zip(model_list, model_names, parameters_list):\n",
        "\n",
        "    random_subset_indices = random.sample(range(len(optX)), 100)\n",
        "    random_optX = [optX[i] for i in random_subset_indices]\n",
        "    random_opty = [opty[i] for i in random_subset_indices]\n",
        "\n",
        "    steps = [\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('pca', PCA(n_components=50)),\n",
        "        ('classifier', model())\n",
        "    ]\n",
        "    pipeline = Pipeline(steps)\n",
        "    grid_search = GridSearchCV(pipeline, parameters, scoring='accuracy')\n",
        "    grid_search.fit(random_optX, random_opty)\n",
        "    print(f'Best paramters for {name} {grid_search.best_params_}')\n",
        "    print(f'Best model for {name} {grid_search.best_estimator_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd9YJ7uCVV2G",
        "outputId": "8b566e95-b34d-4333-98b8-209ff0fb8268"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimization LOWN\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "40 fits failed out of a total of 80.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [ nan 0.76  nan 0.74  nan 0.78  nan 0.74  nan 0.78  nan 0.74  nan 0.78\n",
            "  nan 0.74]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best paramters for Logistic regression {'classifier__C': 1, 'classifier__penalty': 'l2'}\n",
            "Best model for Logistic regression Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', LogisticRegression(C=1))])\n",
            "Best paramters for KNN classif {'classifier__n_neighbors': 25, 'classifier__weights': 'uniform'}\n",
            "Best model for KNN classif Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', KNeighborsClassifier(n_neighbors=25))])\n",
            "Best paramters for RF {'classifier__criterion': 'gini', 'classifier__n_estimators': 200}\n",
            "Best model for RF Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', RandomForestClassifier(n_estimators=200))])\n",
            "Best paramters for AdaBoost {'classifier__learning_rate': 1, 'classifier__n_estimators': 200}\n",
            "Best model for AdaBoost Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier',\n",
            "                 AdaBoostClassifier(learning_rate=1, n_estimators=200))])\n",
            "Best paramters for SVC {'classifier__C': 1, 'classifier__kernel': 'rbf'}\n",
            "Best model for SVC Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', SVC(C=1))])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best paramters for MLP {'classifier__activation': 'relu', 'classifier__hidden_layer_sizes': 500}\n",
            "Best model for MLP Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', MLPClassifier(hidden_layer_sizes=500))])\n"
          ]
        }
      ],
      "source": [
        "optX = lazdef_mlm_PA\n",
        "opty = LazDEF_sample_labels\n",
        "print(\"Optimization LOWN\")\n",
        "\n",
        "model_list = [LogisticRegression, KNeighborsClassifier, RandomForestClassifier, AdaBoostClassifier, SVC, MLPClassifier]\n",
        "model_names = ['Logistic regression', 'KNN classif', 'RF', 'AdaBoost', 'SVC', 'MLP']\n",
        "parameters_list = [\n",
        "    {'classifier__penalty': (\"l1\", \"l2\", \"elasticnet\", None), 'classifier__C':[0.1, 1, 5, 10]},\n",
        "    {'classifier__n_neighbors': [5, 10, 25, 50], 'classifier__weights': ('uniform', 'distance')},\n",
        "    {'classifier__n_estimators': [50, 100, 200, 500], 'classifier__criterion': ('gini', 'entropy', 'log_loss')},\n",
        "    {'classifier__n_estimators': [50, 100, 200, 500], 'classifier__learning_rate': [0.1, 1, 5, 10]},\n",
        "    {'classifier__kernel':('linear', 'rbf'), 'classifier__C':[0.1, 1, 5, 10]},\n",
        "    {'classifier__hidden_layer_sizes': [50, 100, 500, 750, 1000], 'classifier__activation':('tanh', 'relu')},\n",
        "]\n",
        "\n",
        "for model, name, parameters in zip(model_list, model_names, parameters_list):\n",
        "\n",
        "    random_subset_indices = random.sample(range(len(optX)), 100)\n",
        "    random_optX = [optX[i] for i in random_subset_indices]\n",
        "    random_opty = [opty[i] for i in random_subset_indices]\n",
        "\n",
        "    steps = [\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('pca', PCA(n_components=50)),\n",
        "        ('classifier', model())\n",
        "    ]\n",
        "    pipeline = Pipeline(steps)\n",
        "    grid_search = GridSearchCV(pipeline, parameters, scoring='accuracy')\n",
        "    grid_search.fit(random_optX, random_opty)\n",
        "    print(f'Best paramters for {name} {grid_search.best_params_}')\n",
        "    print(f'Best model for {name} {grid_search.best_estimator_}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hupehlaz9a5a"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0IyOvI09bpB",
        "outputId": "79cf64a0-1bcd-4985-9ea6-ce4b9da3a9f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimization MEDN\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "40 fits failed out of a total of 80.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [  nan 0.879   nan 0.879   nan 0.881   nan 0.877   nan 0.881   nan 0.876\n",
            "   nan 0.879   nan 0.877]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best paramters for Logistic regression {'classifier__C': 1, 'classifier__penalty': 'l2'}\n",
            "Best model for Logistic regression Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', LogisticRegression(C=1))])\n",
            "Best paramters for KNN classif {'classifier__n_neighbors': 10, 'classifier__weights': 'uniform'}\n",
            "Best model for KNN classif Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', KNeighborsClassifier(n_neighbors=10))])\n",
            "Best paramters for RF {'classifier__criterion': 'entropy', 'classifier__n_estimators': 500}\n",
            "Best model for RF Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier',\n",
            "                 RandomForestClassifier(criterion='entropy',\n",
            "                                        n_estimators=500))])\n",
            "Best paramters for AdaBoost {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 200}\n",
            "Best model for AdaBoost Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier',\n",
            "                 AdaBoostClassifier(learning_rate=0.1, n_estimators=200))])\n",
            "Best paramters for SVC {'classifier__C': 1, 'classifier__kernel': 'linear'}\n",
            "Best model for SVC Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', SVC(C=1, kernel='linear'))])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best paramters for MLP {'classifier__activation': 'tanh', 'classifier__hidden_layer_sizes': 500}\n",
            "Best model for MLP Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier',\n",
            "                 MLPClassifier(activation='tanh', hidden_layer_sizes=500))])\n"
          ]
        }
      ],
      "source": [
        "optX = lazbf_mlm_PA\n",
        "opty = LazBF_sample_labels\n",
        "print(\"Optimization MEDN\")\n",
        "\n",
        "model_list = [LogisticRegression, KNeighborsClassifier, RandomForestClassifier, AdaBoostClassifier, SVC, MLPClassifier]\n",
        "model_names = ['Logistic regression', 'KNN classif', 'RF', 'AdaBoost', 'SVC', 'MLP']\n",
        "parameters_list = [\n",
        "    {'classifier__penalty': (\"l1\", \"l2\", \"elasticnet\", None), 'classifier__C':[0.1, 1, 5, 10]},\n",
        "    {'classifier__n_neighbors': [5, 10, 25, 50], 'classifier__weights': ('uniform', 'distance')},\n",
        "    {'classifier__n_estimators': [50, 100, 200, 500], 'classifier__criterion': ('gini', 'entropy', 'log_loss')},\n",
        "    {'classifier__n_estimators': [50, 100, 200, 500], 'classifier__learning_rate': [0.1, 1, 5, 10]},\n",
        "    {'classifier__kernel':('linear', 'rbf'), 'classifier__C':[0.1, 1, 5, 10]},\n",
        "    {'classifier__hidden_layer_sizes': [50, 100, 500, 750, 1000], 'classifier__activation':('tanh', 'relu')},\n",
        "]\n",
        "\n",
        "for model, name, parameters in zip(model_list, model_names, parameters_list):\n",
        "\n",
        "    random_subset_indices = random.sample(range(len(optX)), 1000)\n",
        "    random_optX = [optX[i] for i in random_subset_indices]\n",
        "    random_opty = [opty[i] for i in random_subset_indices]\n",
        "\n",
        "    steps = [\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('pca', PCA(n_components=50)),\n",
        "        ('classifier', model())\n",
        "    ]\n",
        "    pipeline = Pipeline(steps)\n",
        "    grid_search = GridSearchCV(pipeline, parameters, scoring='accuracy')\n",
        "    grid_search.fit(random_optX, random_opty)\n",
        "    print(f'Best paramters for {name} {grid_search.best_params_}')\n",
        "    print(f'Best model for {name} {grid_search.best_estimator_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbW1VgqN9hnc",
        "outputId": "dce1f06f-e5da-4913-b22b-047b23d8442b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimization MEDN\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "40 fits failed out of a total of 80.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [ nan 0.7   nan 0.7   nan 0.67  nan 0.7   nan 0.68  nan 0.7   nan 0.68\n",
            "  nan 0.7 ]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best paramters for Logistic regression {'classifier__C': 0.1, 'classifier__penalty': 'l2'}\n",
            "Best model for Logistic regression Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', LogisticRegression(C=0.1))])\n",
            "Best paramters for KNN classif {'classifier__n_neighbors': 25, 'classifier__weights': 'distance'}\n",
            "Best model for KNN classif Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier',\n",
            "                 KNeighborsClassifier(n_neighbors=25, weights='distance'))])\n",
            "Best paramters for RF {'classifier__criterion': 'entropy', 'classifier__n_estimators': 200}\n",
            "Best model for RF Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier',\n",
            "                 RandomForestClassifier(criterion='entropy',\n",
            "                                        n_estimators=200))])\n",
            "Best paramters for AdaBoost {'classifier__learning_rate': 1, 'classifier__n_estimators': 500}\n",
            "Best model for AdaBoost Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier',\n",
            "                 AdaBoostClassifier(learning_rate=1, n_estimators=500))])\n",
            "Best paramters for SVC {'classifier__C': 0.1, 'classifier__kernel': 'linear'}\n",
            "Best model for SVC Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', SVC(C=0.1, kernel='linear'))])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best paramters for MLP {'classifier__activation': 'relu', 'classifier__hidden_layer_sizes': 1000}\n",
            "Best model for MLP Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)),\n",
            "                ('classifier', MLPClassifier(hidden_layer_sizes=1000))])\n"
          ]
        }
      ],
      "source": [
        "optX = lazdef_mlm_PA\n",
        "opty = LazDEF_sample_labels\n",
        "print(\"Optimization MEDN\")\n",
        "\n",
        "model_list = [LogisticRegression, KNeighborsClassifier, RandomForestClassifier, AdaBoostClassifier, SVC, MLPClassifier]\n",
        "model_names = ['Logistic regression', 'KNN classif', 'RF', 'AdaBoost', 'SVC', 'MLP']\n",
        "parameters_list = [\n",
        "    {'classifier__penalty': (\"l1\", \"l2\", \"elasticnet\", None), 'classifier__C':[0.1, 1, 5, 10]},\n",
        "    {'classifier__n_neighbors': [5, 10, 25, 50], 'classifier__weights': ('uniform', 'distance')},\n",
        "    {'classifier__n_estimators': [50, 100, 200, 500], 'classifier__criterion': ('gini', 'entropy', 'log_loss')},\n",
        "    {'classifier__n_estimators': [50, 100, 200, 500], 'classifier__learning_rate': [0.1, 1, 5, 10]},\n",
        "    {'classifier__kernel':('linear', 'rbf'), 'classifier__C':[0.1, 1, 5, 10]},\n",
        "    {'classifier__hidden_layer_sizes': [50, 100, 500, 750, 1000], 'classifier__activation':('tanh', 'relu')},\n",
        "]\n",
        "\n",
        "for model, name, parameters in zip(model_list, model_names, parameters_list):\n",
        "\n",
        "    random_subset_indices = random.sample(range(len(optX)), 1000)\n",
        "    random_optX = [optX[i] for i in random_subset_indices]\n",
        "    random_opty = [opty[i] for i in random_subset_indices]\n",
        "\n",
        "    steps = [\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('pca', PCA(n_components=50)),\n",
        "        ('classifier', model())\n",
        "    ]\n",
        "    pipeline = Pipeline(steps)\n",
        "    grid_search = GridSearchCV(pipeline, parameters, scoring='accuracy')\n",
        "    grid_search.fit(random_optX, random_opty)\n",
        "    print(f'Best paramters for {name} {grid_search.best_params_}')\n",
        "    print(f'Best model for {name} {grid_search.best_estimator_}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
