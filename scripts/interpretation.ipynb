{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "cellView": "form",
        "id": "JIL_EOGpE5Dg"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import torch\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.utils import resample\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.metrics import ndcg_score\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, normalize\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.utils import resample\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "import matplotlib.ticker as ticker\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import ndcg_score\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.svm import SVR, SVC\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "from evaluate import load\n",
        "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "cellView": "form",
        "id": "CsCCBkblFRtG"
      },
      "outputs": [],
      "source": [
        "#@title Load sequences from csv\n",
        "df = pd.read_csv('./LazBF_sequences.csv')\n",
        "LazBF_sequences = df['sequences'].tolist()\n",
        "LazBF_labels = df['labels'].tolist()\n",
        "\n",
        "df = pd.read_csv('./LazBF_sample.csv')\n",
        "LazBF_sample = df['sequences'].tolist()\n",
        "LazBF_sample_labels = df['labels'].tolist()\n",
        "\n",
        "df = pd.read_csv('./LazDEF_sequences.csv')\n",
        "LazDEF_sequences = df['sequences'].tolist()\n",
        "LazDEF_labels = df['labels'].tolist()\n",
        "\n",
        "df = pd.read_csv('./LazDEF_sample.csv')\n",
        "LazDEF_sample = df['sequences'].tolist()\n",
        "LazDEF_sample_labels = df['labels'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "P9L8mx-sLH5T"
      },
      "outputs": [],
      "source": [
        "LazBF_model = AutoModelForSequenceClassification.from_pretrained('./LazBF_ft/checkpoint-9766').to(device).eval()\n",
        "LazDEF_model = AutoModelForSequenceClassification.from_pretrained('./LazDEF_ft/checkpoint-9766').to(device).eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rH3n7uOjFVQh"
      },
      "outputs": [],
      "source": [
        "#@title Trainers\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"esm_finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "    gradient_accumulation_steps=2,\n",
        ")\n",
        "\n",
        "from datasets import load_metric\n",
        "metric = load_metric('accuracy')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "LazBF_trainer = Trainer(\n",
        "    model=LazBF_model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "LazDEF_trainer = Trainer(\n",
        "    model=LazDEF_model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg-3CGwoFYiF"
      },
      "outputs": [],
      "source": [
        "lbf = Dataset.from_dict(tokenizer(LazBF_sample))\n",
        "lbf = lbf.add_column(\"labels\", LazBF_sample_labels)\n",
        "\n",
        "ldef = Dataset.from_dict(tokenizer(LazDEF_sample))\n",
        "ldef = ldef.add_column(\"labels\", LazDEF_sample_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-j9-8fXFaFU"
      },
      "outputs": [],
      "source": [
        "LazBF_trainer.evaluate(lbf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "eiYqJbRCFdWe",
        "outputId": "97e6f28d-a90a-4c0a-e4f2-0ddbb1a114cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='782' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [391/391 00:18]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 3.8547000885009766,\n",
              " 'eval_accuracy': 0.5089,\n",
              " 'eval_runtime': 9.5973,\n",
              " 'eval_samples_per_second': 5209.791,\n",
              " 'eval_steps_per_second': 40.741}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LazBF_trainer.evaluate(ldef)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "4L3fySqDFfUd",
        "outputId": "0efb761b-3ca9-4b66-cc9d-5ba5c55d07b9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='391' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [391/391 00:08]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 1.1414541006088257,\n",
              " 'eval_accuracy': 0.69678,\n",
              " 'eval_runtime': 8.9048,\n",
              " 'eval_samples_per_second': 5614.929,\n",
              " 'eval_steps_per_second': 43.909}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LazDEF_trainer.evaluate(lbf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "cKsQOT5UFgtC",
        "outputId": "afb480a4-1ede-4bb9-e5b2-6c967608e4aa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='782' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [391/391 00:18]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.025831105187535286,\n",
              " 'eval_accuracy': 0.99184,\n",
              " 'eval_runtime': 9.495,\n",
              " 'eval_samples_per_second': 5265.948,\n",
              " 'eval_steps_per_second': 41.18}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LazDEF_trainer.evaluate(ldef)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EIFZlNsFk6-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "yy3cQmBUFlR1"
      },
      "outputs": [],
      "source": [
        "#@title Interpretability\n",
        "\n",
        "from torch import tensor\n",
        "import matplotlib.colors as mcolors\n",
        "from transformers.pipelines import TextClassificationPipeline\n",
        "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, IntegratedGradients\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "sm = torch.nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "class ExplainableTransformerPipeline():\n",
        "    def __init__(self, name, device):\n",
        "        self.__name = name\n",
        "        self.__device = device\n",
        "\n",
        "    def forward_func(self, inputs: tensor, position = 0):\n",
        "        pred = model.forward(inputs)\n",
        "        return pred[position]\n",
        "\n",
        "    def visualize(self, inputs: list, attributes: list, prediction):\n",
        "        attr_sum = attributes.sum(-1)\n",
        "        attr = attr_sum #/ torch.norm(attr_sum)\n",
        "        attr = [float(at) for at in attr_sum[0]]\n",
        "        y = np.array(attr)\n",
        "        #y*=-1\n",
        "        #a = pd.Series(attr.numpy()[0], index = tokenizer.convert_ids_to_tokens(inputs.detach().numpy()[0]))\n",
        "        peptide = tokenizer(sample).input_ids #[1:-1] #list(sample)\n",
        "        words = [tokenizer.decode(i) for i in peptide]\n",
        "        #return y\n",
        "        #print(y)\n",
        "        #cmap = plt.cm.get_cmap('cividis') # Use the 'viridis' colormap\n",
        "        #norm = plt.Normalize(min(y), max(y))\n",
        "\n",
        "\n",
        "        letters = np.array(words[1:-1])\n",
        "        colors = np.array([mcolors.to_hex(cmap(norm(datapoint))) for datapoint in y[1:-1]])\n",
        "        positions = np.arange(len(letters))\n",
        "\n",
        "        print(\"Model's prediction:\", str(float(prediction[0][0])))\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        for i, letter in enumerate(letters):\n",
        "          rect = plt.Rectangle((positions[i], 0), 1, 1, color=colors[i])\n",
        "          ax.add_patch(rect)\n",
        "          plt.text(positions[i]+0.5, 0.5, letter, ha='center', va='center', fontsize=100)\n",
        "\n",
        "        ax.set_xlim([0, len(letters)])\n",
        "        ax.set_ylim([0, 1])\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        fig.set_size_inches(200, 10)\n",
        "        plt.show()\n",
        "\n",
        "    def explain(self, text: str, label):\n",
        "        inputs = torch.tensor(tokenizer.encode(text, add_special_tokens=True), device=self.__device).unsqueeze(0)\n",
        "        prediction = model.forward(inputs)[0]\n",
        "        baseline = torch.tensor([tokenizer.cls_token_id] + [tokenizer.pad_token_id] * (inputs.shape[1] - 2) + [tokenizer.eos_token_id], device = self.__device).unsqueeze(0)\n",
        "        lig = LayerIntegratedGradients(self.forward_func, model.esm.embeddings) # The 'layer' from which we want to get the IG's is the mebedding slyaer.\n",
        "        attributes, delta = lig.attribute(inputs=inputs,\n",
        "                                  baselines=baseline,\n",
        "                                  target = label,\n",
        "                                  return_convergence_delta = True)\n",
        "        attr_sum = attributes.sum(-1)\n",
        "        return attr_sum.cpu().numpy()[0][1:-1]\n",
        "        #return self.visualize(inputs, attributes, prediction) #float(sm(prediction)[0][1]))\n",
        "\n",
        "    def generate_inputs(self, text: str) -> tensor:\n",
        "        return torch.tensor(tokenizer.encode(text, add_special_tokens=True), device=self.__device).unsqueeze(0)\n",
        "\n",
        "    def generate_baseline(self, sequence_len: int) -> tensor:\n",
        "        return torch.tensor([tokenizer.cls_token_id] + [tokenizer.pad_token_id] * (sequence_len - 2) + [tokenizer.eos_token_id], device = self.__device).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "Wo7Zri-LFpQg"
      },
      "outputs": [],
      "source": [
        "#@title Amino acid dictionary\n",
        "amino_acids = {\n",
        "    'R': 0,\n",
        "    'H': 1,\n",
        "    'K': 2,\n",
        "    'D': 3,\n",
        "    'E': 4,\n",
        "    'S': 5,\n",
        "    'T': 6,\n",
        "    'N': 7,\n",
        "    'Q': 8,\n",
        "    'C': 9,\n",
        "    'G': 10,\n",
        "    'P': 11,\n",
        "    'A': 12,\n",
        "    'V': 13,\n",
        "    'I': 14,\n",
        "    'L': 15,\n",
        "    'M': 16,\n",
        "    'F': 17,\n",
        "    'Y': 18,\n",
        "    'W': 19,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1ZyLNAlFsZQ",
        "outputId": "24155311-8d76-403b-9043-644c7c1d7efe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [1:52:10<00:00,  7.43it/s]\n",
            "<ipython-input-14-c80985e5b9b9>:26: RuntimeWarning: invalid value encountered in divide\n",
            "  avg_position_contribBF = avg_position_contribBF / avg_position_contribBF_counter\n"
          ]
        }
      ],
      "source": [
        "# Empty lists for storing avg contributions\n",
        "avg_contribBF = np.zeros((20,))\n",
        "avg_positionBF = np.zeros((11,))\n",
        "avg_position_contribBF = np.zeros((20, 11))\n",
        "\n",
        "avg_position_contribBF_counter = np.zeros((20, 11))\n",
        "\n",
        "# Define model and aa_counter\n",
        "model = LazBF_model\n",
        "exp_model = ExplainableTransformerPipeline('distilbert', device)\n",
        "aa_counts = np.zeros((20,))\n",
        "\n",
        "for peptide in tqdm(LazBF_sample):\n",
        "  contributions = exp_model.explain(peptide, 1)\n",
        "  for i, letter in enumerate(peptide):\n",
        "    # Add to position avg\n",
        "    avg_positionBF[i] += contributions[i]\n",
        "    # Add to pos x AA average\n",
        "    avg_position_contribBF[amino_acids[letter]][i] += contributions[i]\n",
        "    avg_position_contribBF_counter[amino_acids[letter]][i] += 1\n",
        "    # Add to aa average\n",
        "    avg_contribBF[amino_acids[letter]] += contributions[i]\n",
        "    # Count amino acid types\n",
        "    aa_counts[amino_acids[letter]] += 1\n",
        "\n",
        "avg_position_contribBF = avg_position_contribBF / avg_position_contribBF_counter\n",
        "avg_contribBF = avg_contribBF / aa_counts\n",
        "avg_positionBF = avg_positionBF / 50000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YMTLIMkkFxhJ"
      },
      "outputs": [],
      "source": [
        "# np.save('./drive/MyDrive/avg_pos_contribBF', avg_positionBF)\n",
        "# np.save('./drive/MyDrive/avg_aa_contribBF', avg_contribBF)\n",
        "# np.save('./drive/MyDrive/avg_posxaa_contribBF', avg_position_contribBF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNNESh4FFznr",
        "outputId": "e9e5a39a-7d03-44a5-dc6f-9629af62a734"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [1:50:43<00:00,  7.53it/s]\n",
            "<ipython-input-16-651e73bd1309>:25: RuntimeWarning: invalid value encountered in divide\n",
            "  avg_position_contribDEF = avg_position_contribDEF / avg_position_contribDEF_count\n"
          ]
        }
      ],
      "source": [
        "# Empty lists for storing avg contributions\n",
        "avg_contribDEF = np.zeros((20,))\n",
        "avg_positionDEF = np.zeros((11,))\n",
        "avg_position_contribDEF = np.zeros((20, 11))\n",
        "avg_position_contribDEF_count = np.zeros((20, 11))\n",
        "\n",
        "# Define model and aa_counter\n",
        "model = LazDEF_model\n",
        "exp_model = ExplainableTransformerPipeline('distilbert', device)\n",
        "aa_counts = np.zeros((20,))\n",
        "\n",
        "for peptide in tqdm(LazBF_sample):\n",
        "  contributions = exp_model.explain(peptide, 1)\n",
        "  for i, letter in enumerate(peptide):\n",
        "    # Add to position avg\n",
        "    avg_positionDEF[i] += contributions[i]\n",
        "    # Add to pos x AA average\n",
        "    avg_position_contribDEF[amino_acids[letter]][i] += contributions[i]\n",
        "    avg_position_contribDEF_count[amino_acids[letter]][i] += 1\n",
        "    # Add to aa average\n",
        "    avg_contribDEF[amino_acids[letter]] += contributions[i]\n",
        "    # Count amino acid types\n",
        "    aa_counts[amino_acids[letter]] += 1\n",
        "\n",
        "avg_position_contribDEF = avg_position_contribDEF / avg_position_contribDEF_count\n",
        "avg_contribDEF = avg_contribDEF / aa_counts\n",
        "avg_positionDEF = avg_positionDEF / 50000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aRg1zuLfF0BT"
      },
      "outputs": [],
      "source": [
        "# np.save('./drive/MyDrive/avg_pos_contribDEF', avg_positionDEF)\n",
        "# np.save('./drive/MyDrive/avg_aa_contribDEF', avg_contribDEF)\n",
        "# np.save('./drive/MyDrive/avg_posxaa_contribDEF', avg_position_contribDEF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GJwgFSMDF19Y"
      },
      "outputs": [],
      "source": [
        "avg_positionBF = np.load('./drive/MyDrive/avg_pos_contribBF.npy')\n",
        "avg_contribBF = np.load('./drive/MyDrive/avg_aa_contribBF.npy')\n",
        "avg_position_contribBF = np.load('./drive/MyDrive/avg_posxaa_contribBF.npy')\n",
        "\n",
        "avg_positionDEF = np.load('./drive/MyDrive/avg_pos_contribDEF.npy')\n",
        "avg_contribDEF = np.load('./drive/MyDrive/avg_aa_contribDEF.npy')\n",
        "avg_position_contribDEF = np.load('./drive/MyDrive/avg_posxaa_contribDEF.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7JRNBJ_F5HZ",
        "outputId": "7406178d-2761-4558-b74d-d2cae0fb89dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SignificanceResult(statistic=0.8105263157894737, pvalue=1.4687607586992212e-05)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from scipy.stats import spearmanr\n",
        "spearmanr(np.nan_to_num(avg_contribBF, nan=0.0), np.nan_to_num(avg_contribDEF, nan=0.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JrQ3B7nF8Av",
        "outputId": "87385c50-1824-4270-8c3e-227a679ef109"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SignificanceResult(statistic=0.8000000000000002, pvalue=0.0031104283103858483)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from scipy.stats import spearmanr\n",
        "spearmanr(np.nan_to_num(avg_positionDEF, nan=0.0), np.nan_to_num(avg_positionBF, nan=0.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-B3FtWKF-Qj",
        "outputId": "90126211-7394-41d1-c74c-da35ee10631e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SignificanceResult(statistic=0.5867492909461007, pvalue=9.61098530273787e-22)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from scipy.stats import spearmanr\n",
        "spearmanr(np.nan_to_num(avg_position_contribBF, nan=0.0).flatten(), np.nan_to_num(avg_position_contribDEF, nan=0.0).flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "cellView": "form",
        "id": "t2qFsb87wiCZ"
      },
      "outputs": [],
      "source": [
        "#@title Helper functions\n",
        "csfont = {'fontname':'Times New Roman'}\n",
        "\n",
        "def full_last_layer(model, sequence):\n",
        "  input = tokenizer(sequence, return_tensors='pt').to(device)\n",
        "  output = model.forward(input.input_ids, output_attentions=True)\n",
        "  matrs = []\n",
        "  for i, head in enumerate(output.attentions[-1][0]):\n",
        "      matr = head.cpu().detach().numpy()\n",
        "      matrs.append(matr)\n",
        "      plt.imshow(matr, interpolation='nearest')\n",
        "      x_ticks = np.arange(0, matr.shape[1])\n",
        "      x_tick_labels = [\"[BOS]\"] + list(sequence) + [\"[EOS]\"]\n",
        "      plt.xticks(x_ticks, x_tick_labels)\n",
        "      y_ticks = np.arange(0, matr.shape[1])\n",
        "      y_tick_labels = [\"[BOS]\"] + list(sequence) + [\"[EOS]\"]\n",
        "      sizes = [9] + 13*[14] + [9]\n",
        "      plt.yticks(y_ticks, y_tick_labels)\n",
        "      for j, label in enumerate(plt.xticks()[1]):\n",
        "        label.set_fontsize(sizes[j])\n",
        "      for j, label in enumerate(plt.yticks()[1]):\n",
        "        label.set_fontsize(sizes[j])\n",
        "      plt.title(f\"Layer 12, Head {i+1}\", fontsize=17)\n",
        "      plt.colorbar()\n",
        "      #plt.text(-4, -0.9, 'c', fontsize=20) #, transform=ax.transAxes)\n",
        "      plt.savefig(f'./VIGGRTCDGTRYY_head_{i+1}_alt.png', dpi=400, bbox_inches='tight', pad_inches=0)\n",
        "      plt.show()\n",
        "def full_attention(model, sequence):\n",
        "  input = tokenizer(sequence, return_tensors='pt').to(device)\n",
        "  output = model.forward(input.input_ids, output_attentions=True)\n",
        "  matrs = []\n",
        "  for layer, att in enumerate(output.attentions):\n",
        "    for i, head in enumerate(att[0]):\n",
        "      print(f'Layer {layer}, head {i}')\n",
        "      matr = head.cpu().detach().numpy()\n",
        "      matrs.append(matr)\n",
        "      plt.imshow(matr, interpolation='nearest')\n",
        "      x_ticks = np.arange(0, matr.shape[1])\n",
        "      x_tick_labels = list(\" \"+sequence+\" \")\n",
        "      plt.xticks(x_ticks, x_tick_labels)\n",
        "      y_ticks = np.arange(0, matr.shape[1])\n",
        "      y_tick_labels = list(\" \"+sequence+\" \")\n",
        "      plt.yticks(y_ticks, y_tick_labels)\n",
        "      plt.show()\n",
        "\n",
        "def per_layer_attention(model, sequence):\n",
        "  input = tokenizer(sequence, return_tensors='pt').to(device)\n",
        "  output = model.forward(input.input_ids, output_attentions=True)\n",
        "  for i, att in enumerate(output.attentions):\n",
        "    matrs = []\n",
        "    for head in att[0]:\n",
        "      matr = head.cpu().detach().numpy()\n",
        "      matrs.append(matr)\n",
        "    matrs = np.array(matrs)\n",
        "    plt.imshow(np.mean(matrs, axis=0), interpolation='nearest', vmin=0, vmax=0.3)\n",
        "    x_ticks = np.arange(0, matr.shape[1])\n",
        "    x_tick_labels = [\"[BOS]\"] + list(sequence) + [\"[EOS]\"]\n",
        "\n",
        "    plt.xticks(x_ticks, x_tick_labels)\n",
        "    sizes = [10] + 11*[15] + [10]\n",
        "    for j, label in enumerate(plt.xticks()[1]):\n",
        "      label.set_fontsize(sizes[j])\n",
        "\n",
        "    y_ticks = np.arange(0, matr.shape[1])\n",
        "    y_tick_labels = [\"[BOS]\"] + list(sequence) + [\"[EOS]\"]\n",
        "    plt.yticks(y_ticks, y_tick_labels)\n",
        "    sizes = [10] + 11*[15] + [10]\n",
        "    for j, label in enumerate(plt.yticks()[1]):\n",
        "      label.set_fontsize(sizes[j])\n",
        "\n",
        "    plt.title(f'Average Attention for Layer {i+1}', fontsize=17)\n",
        "    #plt.text(-4, -0.9, 'a', fontsize=20) #, transform=ax.transAxes)\n",
        "    plt.colorbar()\n",
        "\n",
        "    plt.savefig(f'./FVCHPSRWVGA_layer_{i+1}_alt.png', dpi=400, bbox_inches='tight', pad_inches=0)\n",
        "    plt.show()\n",
        "\n",
        "def oned_attention(model, sequence):\n",
        "  input = tokenizer(sequence, return_tensors='pt').to(device)\n",
        "  output = model.forward(input.input_ids, output_attentions=True)\n",
        "  for att in output.attentions:\n",
        "    matrs = []\n",
        "    for head in att[0]:\n",
        "      matrs.append(head.cpu().detach().numpy())\n",
        "    matrs = np.array(matrs)\n",
        "    matrs = np.mean(matrs, axis=0)\n",
        "    matrs = np.mean(matrs, axis=0)\n",
        "    print(matrs[:, np.newaxis].T)\n",
        "    plt.imshow(matrs[:, np.newaxis].T, interpolation='nearest')\n",
        "    x_ticks = np.arange(0, len(sequence)+2)\n",
        "    x_tick_labels = list(\" \"+sequence+\" \")\n",
        "    plt.xticks(x_ticks, x_tick_labels)\n",
        "    plt.show()\n",
        "\n",
        "def layer11(model, sequence):\n",
        "  input = tokenizer(sequence, return_tensors='pt').to(device)\n",
        "  output = model.forward(input.input_ids, output_attentions=True)\n",
        "  matrs = []\n",
        "  for i, head in enumerate(output.attentions[-2][0]):\n",
        "      matr = head.cpu().detach().numpy()\n",
        "      matrs.append(matr)\n",
        "      plt.imshow(matr, interpolation='nearest', vmin=0, vmax=0.8)\n",
        "      x_ticks = np.arange(0, matr.shape[1])\n",
        "      x_tick_labels = [\"[BOS]\"] + list(sequence) + [\"[EOS]\"]\n",
        "      plt.xticks(x_ticks, x_tick_labels)\n",
        "      y_ticks = np.arange(0, matr.shape[1])\n",
        "      y_tick_labels = [\"[BOS]\"] + list(sequence) + [\"[EOS]\"]\n",
        "      plt.yticks(y_ticks, y_tick_labels)\n",
        "\n",
        "      sizes = [10] + 11*[15] + [10]\n",
        "      for j, label in enumerate(plt.xticks()[1]):\n",
        "        label.set_fontsize(sizes[j])\n",
        "      for j, label in enumerate(plt.yticks()[1]):\n",
        "        label.set_fontsize(sizes[j])\n",
        "\n",
        "      plt.title(f\"Layer 11, Head {i+1}\", fontsize=17)\n",
        "      #plt.colorbar()\n",
        "      plt.text(-4, -0.9, 'b', fontsize=20) #, transform=ax.transAxes)\n",
        "      plt.savefig(f'./FVCHPSRWVGA_head_{i+1}_alt.png', dpi=400, bbox_inches='tight', pad_inches=0)\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIZ1RlhDwoBz"
      },
      "outputs": [],
      "source": [
        "per_layer_attention(LazBF_model, \"FVCHPSRWVGA\") #'VIGGRTCDGTRYY')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
