{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sWQa-3bKq1uu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76b09557-e4e1-4a78-f19c-cc43a0d8a459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrbskvxRp4mV",
        "outputId": "8a0e83db-f458-4989-cb9e-457fa50845d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4851291/4851291 [00:25<00:00, 190933.97it/s]\n",
            "100%|██████████| 5625806/5625806 [00:30<00:00, 182792.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4500557/4500557 [00:28<00:00, 160347.95it/s]\n",
            "100%|██████████| 4314135/4314135 [00:25<00:00, 168507.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Preprocesses the LazBF/DEF data as described in the Data Preprocessing section\n",
        "# Creates the LazBF/DEF MLM and held-out  data sets\n",
        "\n",
        "#Imports\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.utils import resample\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.metrics import ndcg_score\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, normalize\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.utils import resample\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import ndcg_score\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.svm import SVR, SVC\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "\n",
        "# Load all LazBF/DEF sequences as .npy files\n",
        "LazBF_neg = np.load('./drive/MyDrive/Data/55_r6_anti_P.npy')\n",
        "LazBF_pos = np.load('./drive/MyDrive/Data/55_r6_sele_P.npy')\n",
        "LazDEF_neg = np.load('./drive/MyDrive/Data/66_r5_anti_P.npy')\n",
        "LazDEF_pos = np.load('./drive/MyDrive/Data/66_r5_sele_P.npy')\n",
        "\n",
        "# Convert all LazBF sequences to strings\n",
        "LazBF_sequences_pos = []\n",
        "LazBF_sequences_neg = []\n",
        "\n",
        "for i in tqdm(range(LazBF_pos.shape[0])):\n",
        "  peptide = ''.join([str(aa)[2] for aa in LazBF_pos[i]])\n",
        "  LazBF_sequences_pos.append(peptide)\n",
        "\n",
        "for i in tqdm(range(LazBF_neg.shape[0])):\n",
        "  peptide = ''.join([str(aa)[2] for aa in LazBF_neg[i]])\n",
        "  LazBF_sequences_neg.append(peptide)\n",
        "\n",
        "# Remove duplicates\n",
        "LazBF_sequences_pos = list(set(LazBF_sequences_pos))\n",
        "LazBF_sequences_neg = list(set(LazBF_sequences_neg))\n",
        "\n",
        "# Remove sequences found in both the selection and antiselection data\n",
        "common_seqs = set(LazBF_sequences_pos) & set(LazBF_sequences_neg)\n",
        "LazBF_sequences_pos = [seq for seq in LazBF_sequences_pos if seq not in common_seqs]\n",
        "LazBF_sequences_neg = [seq for seq in LazBF_sequences_neg if seq not in common_seqs]\n",
        "\n",
        "# Create the list of labels\n",
        "LazBF_pos_labels = [1] * len(LazBF_sequences_pos)\n",
        "LazBF_neg_labels = [0] * len(LazBF_sequences_neg)\n",
        "\n",
        "# Balanced sample of 1300000 sequences\n",
        "LazBF_pos, _, LazBF_pos_labels, _ = train_test_split(LazBF_sequences_pos, LazBF_pos_labels, train_size=int(1300000/2), random_state=42)\n",
        "LazBF_neg, _, LazBF_neg_labels, _ = train_test_split(LazBF_sequences_neg, LazBF_neg_labels, train_size=int(1300000/2), random_state=42)\n",
        "LazBF_sequences = LazBF_pos + LazBF_neg\n",
        "LazBF_labels = LazBF_pos_labels + LazBF_neg_labels\n",
        "print(len(LazBF_sequences) == len(set(LazBF_sequences)))\n",
        "\n",
        "# Convert all LazDEF sequences to strings\n",
        "LazDEF_sequences_pos = []\n",
        "LazDEF_sequences_neg = []\n",
        "\n",
        "for i in tqdm(range(LazDEF_pos.shape[0])):\n",
        "  peptide = ''.join([str(aa)[2] for aa in LazDEF_pos[i]])\n",
        "  LazDEF_sequences_pos.append(peptide)\n",
        "\n",
        "for i in tqdm(range(LazDEF_neg.shape[0])):\n",
        "  peptide = ''.join([str(aa)[2] for aa in LazDEF_neg[i]])\n",
        "  LazDEF_sequences_neg.append(peptide)\n",
        "\n",
        "# Remove duplicates\n",
        "LazDEF_sequences_pos = list(set(LazDEF_sequences_pos))\n",
        "LazDEF_sequences_neg = list(set(LazDEF_sequences_neg))\n",
        "\n",
        "# Remove sequences found in both the selection and antiselection data\n",
        "common_seqs = set(LazDEF_sequences_pos) & set(LazDEF_sequences_neg)\n",
        "LazDEF_sequences_pos = [seq for seq in LazDEF_sequences_pos if seq not in common_seqs]\n",
        "LazDEF_sequences_neg = [seq for seq in LazDEF_sequences_neg if seq not in common_seqs]\n",
        "\n",
        "# Create the list of labels\n",
        "LazDEF_pos_labels = [1] * len(LazDEF_sequences_pos)\n",
        "LazDEF_neg_labels = [0] * len(LazDEF_sequences_neg)\n",
        "\n",
        "# Balanced sample of 1300000 sequences\n",
        "LazDEF_pos, _, LazDEF_pos_labels, _ = train_test_split(LazDEF_sequences_pos, LazDEF_pos_labels, train_size=int(1300000/2), random_state=42)\n",
        "LazDEF_neg, _, LazDEF_neg_labels, _ = train_test_split(LazDEF_sequences_neg, LazDEF_neg_labels, train_size=int(1300000/2), random_state=42)\n",
        "LazDEF_sequences = LazDEF_pos + LazDEF_neg\n",
        "LazDEF_labels = LazDEF_pos_labels + LazDEF_neg_labels\n",
        "print(len(LazDEF_sequences) == len(set(LazDEF_sequences)))\n",
        "\n",
        "# Create MLM and held-out sets\n",
        "LazBF_sequences, LazBF_sample, LazBF_labels, LazBF_sample_labels = train_test_split(LazBF_sequences, LazBF_labels, train_size=len(LazBF_sequences)-50000, test_size=50000, stratify=LazBF_labels, random_state=42)\n",
        "LazDEF_sequences, LazDEF_sample, LazDEF_labels, LazDEF_sample_labels = train_test_split(LazDEF_sequences, LazDEF_labels, train_size=len(LazDEF_sequences)-50000, test_size=50000, stratify=LazDEF_labels, random_state=42)\n",
        "\n",
        "# Save each data set to csv\n",
        "df = pd.DataFrame({'sequences': LazBF_sequences, 'labels': LazBF_labels})\n",
        "df.to_csv('./drive/MyDrive/Data/LazBF_sequences.csv', index=False)\n",
        "\n",
        "df = pd.DataFrame({'sequences': LazBF_sample, 'labels': LazBF_sample_labels})\n",
        "df.to_csv('./drive/MyDrive/Data/LazBF_sample.csv', index=False)\n",
        "\n",
        "df = pd.DataFrame({'sequences': LazDEF_sequences, 'labels': LazDEF_labels})\n",
        "df.to_csv('./drive/MyDrive/Data/LazDEF_sequences.csv', index=False)\n",
        "\n",
        "df = pd.DataFrame({'sequences': LazDEF_sample, 'labels': LazDEF_sample_labels})\n",
        "df.to_csv('./drive/MyDrive/Data/LazDEF_sample.csv', index=False)"
      ]
    }
  ]
}